# ğŸ§  Redes Neuronales para MNIST

Uno de los primeros proyectos de redes neuronales, perfecto para empezar a entenderlas.

Este proyecto implementa una red neuronal desde cero para clasificar dÃ­gitos escritos a mano del dataset MNIST. 
EstÃ¡ desarrollado en un notebook de Jupyter utilizando **NumPy**, lo que lo convierte en una excelente herramienta didÃ¡ctica para entender los fundamentos del aprendizaje profundo.

## ğŸ“˜ DescripciÃ³n general

- ImplementaciÃ³n manual de capas: `Linear`, `ReLU`, `Flatten`, `Input`.
- PropagaciÃ³n hacia adelante (`forward`) y hacia atrÃ¡s (`backward`) sin frameworks externos.
- Entrenamiento con codificaciÃ³n one-hot y funciÃ³n de activaciÃ³n `softmax`.
- EvaluaciÃ³n con precisiÃ³n y matriz de confusiÃ³n.
- VisualizaciÃ³n con `matplotlib` y `seaborn`.

## ğŸ—‚ï¸ Dataset

Este es el dataset de MNIST usado con este Notebook: https://www.kaggle.com/datasets/hojjatk/mnist-dataset
Tener en cuenta que por defecto en el Notebook este dataset estÃ¡ dentro de una carpeta llamada ``dataset``

## ğŸ§  Arquitectura del modelo

- Entrada: 784 neuronas (28Ã—28 pÃ­xeles)
- Capa oculta: 100 neuronas con activaciÃ³n ``ReLU``
- Salida: 10 neuronas (una por dÃ­gito)
- ActivaciÃ³n final: ``softmax``

## ğŸ“ˆ Entrenamiento

-FunciÃ³n de pÃ©rdida: MSE 
-OptimizaciÃ³n manual con gradiente descendente
-Coste por Ã©poca impreso en consola

## âš™ï¸ Requisitos

Instala las dependencias necesarias:
```bash
pip install -r requirements.txt
```

## ğŸ““ Ejecuta el Notebook
Abre el notebook desde Jupyter:
```bash
jupyter notebook Transformer_Sentiment_Analysis.ipynb
```

## â¬†ï¸ Posibles Mejoras
Implementar ``cross-entropy`` como funciÃ³n de pÃ©rdida

AÃ±adir mÃ¡s capas ocultas o regularizaciÃ³n

Visualizar ejemplos mal clasificados

Guardar pesos del modelo y reutilizarlos

## ğŸ“„ Licencia

Proyecto bajo la licencia MIT. Ver el archivo LICENSE para mÃ¡s detalles.
